{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos básicos de redes neuronales\n",
    "\n",
    "**Carpeta de trabajo**: https://drive.google.com/drive/u/0/folders/1Nl-8QVK1PGDCIePi6-M60mHthSsbXMbP\n",
    "\n",
    "**Videos:** https://www.youtube.com/watch?v=MyrjuBGyr90&list=PLi3X2PHYk7zQUnOITsQJckF1_yHDsJjqT&index=4 \n",
    "\n",
    "## Perceptrón:\n",
    "- Fue introducido por Rossenblar a finales de los años 50\n",
    "- Se inspira en los procesos de aprendizaje de los animales en los cuales la información va atravesando diferentes capas de neuronas\n",
    "- Es un modelo unidireccional, compuesto por dos capas de neuronas, una de entrada y otra de salida\n",
    "- La operación de este tipo puede darse con _n_ neuronas de entrada y _m_ de salida.\n",
    "\n",
    "### Definición\n",
    "- Las neuronas de entrada no realizan ningún computo\n",
    "- Se consideran señales discretas 0 o 1\n",
    "- La operación para _n_ neuoronas de entrada y _m_ de salida puede considerarse así:\n",
    "$$y_i = H(\\sum_{j=1}^{n}w_{ij}x_{j}-\\Theta_{i}), \\forall i, 1<= i <=m$$\n",
    "\n",
    "Donde H(x) es la función escalón. $w_{ij}$ los pesos, x_{j} las entradas que recibe cada estimulo y $\\Theta_{i}$ es el umbral.\n",
    "\n",
    "- El perceptrón permite clasificar dos conjuntos linealmente separables en un plano o hiperplano. \n",
    "- La respuesra de la neurona es 1 si pertenece a la clase o 0 si no pertenece\n",
    "\n",
    "**Ejemplo de prueba**\n",
    "- Sea una neurona tipo perceptrón con entrada $x_1$ y $x_2$\n",
    "- Entonces la operación se define como\n",
    "\n",
    "$$y = H(w_1 x_1 + w_2 x_2 - \\Theta)$$\n",
    "\n",
    "En este caso la función es lineal, muestra entonces una recta que divide el plano en dos regiones. Por lo tanto, se requiere que el problema de clasificación (binaria) requiere una solución lineal: or y and se pueden resolver con perceptrón, pero xor no. Podría resolver problemas mas complejos, como xor, pero para ello requiero mas de una capa. Esto es un perceptrón de dos capas. La complejidad se puede ampliar con las capas, permitiendo regiones convexas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Algoritmo de aprendizaje del perceptrón\n",
    "\n",
    "- La idea principal del entrenamiento es es minimizar los errores, esto es la diferencia entre la estimación $\\hat{y}$ y lo observado $y$.\n",
    "- Vamos a introducir una tasa de aprendizaje _n_ que indica el ritmo de aprendizaje. Si la tasa de aprendizaje es mul alta es posible que no se encuentre una solución, ya que no logra una solución convergente y empieza a oscilar. Si es muy pequeño, el entrenamiento se demoraría demasiado. \n",
    "- Dados unos patrones $x^u$, salidas obtenidas $y^u$ y salidas deseadas $t^u$\n",
    "- Los pesos iniciales son aleatorios entre -1 y 1\n",
    "- Se examina cada patrón y aplicamos la relación de cambio\n",
    "\n",
    "$$\\triangle w_{ij}^{u}(t)=n(t_i ^u - y_i ^u)x_j ^u$$\n",
    "\n",
    "A esto se le conoce como **regla del perceptrón**\n",
    "\n",
    "La idea es ajustar los pesos para poder realizar la separación lineal que nos interesa. Acá estamos considerando un problema de clasificación lineal, que son los problemas que resuelven los **perceptrónes monocapa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de aprendizaje\n",
    "\n",
    "Desarrollaremos un algoritmo de clasificación entre 0 y 1.\n",
    "\n",
    "1. Inicializamos los pesos aleatoriamente entre [-1,1]\n",
    "2. Para el estado _t_, $\\hat{y}$ o salida deseada. Calcular:\n",
    "\n",
    "$$\\hat{y} = t^{u}(k)=H(\\sum_{j=1}^n (x_j j)-\\Theta)$$\n",
    "\n",
    "3. Coregir pesos sinápticos (si $\\hat{y} \\neq y$)\n",
    "\n",
    "$$w_{j} = w_{j} + n[\\hat{y} - y]x$$\n",
    "\n",
    "4. Para si no se han modificado los pesos en los últimos p patrones o se ha llegado a un número de iteraciones especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo perceptrón en Python\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos función de activación escalonada, la cual deseamos que opere sobre estructuras iterables\n",
    "\n",
    "# Implementación explícita sobre arreglos\n",
    "def activacion(x):\n",
    "    salida = []\n",
    "    for i in x:\n",
    "        salida.append(1 if i>= 0 else 0) # Operador ternario\n",
    "    return salida\n",
    "\n",
    "# Implementación vectorizada\n",
    "\n",
    "def activacion_no_vec(x):\n",
    "    return 1 if x>= 0 else 0 \n",
    "# Vectorización de la función de activación\n",
    "activacion_v = np.vectorize(activacion_no_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementación explícita con bucles: [1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "Implementación vectorizada: [1 0 1 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Veamos ejemplos de como operaría la función escalonada\n",
    "\n",
    "random_list = np.random.randint(-10,10,10)\n",
    "\n",
    "print(f'Implementación explícita con bucles: {activacion(random_list)}') #lista\n",
    "print(f'Implementación vectorizada: {activacion_v(random_list)}') # np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la neurona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurona(input,weight,bias):\n",
    "    net = np.dot(weight,input.T)-bias \n",
    "    return activacion_v(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La salida es un escalar, juguemos con valores de i,w y b para ver la cuestión\n",
    "\n",
    "neurona(np.array([-1,2,-3,3.9]),np.array([1,1,1,1]),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(input, output, n, n_patterns=10, max_it=1000):\n",
    "    # Inicializo los pesos y el sesgo aleatoriamente\n",
    "    weights = 2 * np.random.rand(input.shape[1]) - 1  # El producto por dos y menos 1 es para que quede entre el intervalo deseado [-1,1]\n",
    "    bias = 2 * np.random.rand(1) - 1\n",
    "\n",
    "    p = 0 # Número de patrones clasificados correctamente en una iretación\n",
    "    it = 0 # Iteraciones\n",
    "    loss = [] # Error cuadratico medio sum(e^2)/n\n",
    "\n",
    "    # Condición de terminación del entrenamiento\n",
    "    while (p <= n_patterns and it < max_it):\n",
    "        it += 1\n",
    "        p = 0  # Reinicio el contador de patrones clasificados correctamente\n",
    "        for i, t in zip(input, output): # zip agrupa elementos en parejas ordenados (input[0], output[0]), por ejemplo. esta forma del for es común con parejas ordenadas\n",
    "            y = neurona(i, weights, bias) # Salida del perceptron \n",
    "            error = t - y[0]\n",
    "            error_it += error**2\n",
    "            p+=1\n",
    "\n",
    "            # Lo que sigue es el algoritmo de correción\n",
    "\n",
    "            if error != 0:\n",
    "                p = 0\n",
    "                weights = weights + n * error * inp\n",
    "                bias = bias + n * error * (-1)\n",
    "\n",
    "        error_it /= ents.shape[0]\n",
    "        loss.append(error_it) \n",
    "\n",
    "    return weights, bias, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de nuestro perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generemos un data set de prueba\n",
    "\n",
    "X = np.array([[x,y,z,w,a] for x in range(0,2)\n",
    "                   for y in range(0,2)\n",
    "                   for z in range(0,2)\n",
    "                   for w in range(0,2)\n",
    "                   for a in range(0,2)]\n",
    ")\n",
    "\n",
    "# Probemos primero con la función or\n",
    "\n",
    "y = np.array(list(map(lambda e: e[0] or e[1] or e[2] or e[3] or e[4],ents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización del conjunto de datos\n",
    "\n",
    "list(zip(entr, sali))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los hiperparámetros del modelo\n",
    "n = 0.5 # factor de aprendizaje\n",
    "max_it = 200 \n",
    "p = len(sali) #Cuántos patrones debe contar (debe contar que todos estén bien, por eso es 2**5). No hay error.\n",
    "weight,bias = perceptron(entr,sali,n,p,max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0] 0 1\n",
      "[0 0 0 0 1] 1 1\n",
      "[0 0 0 1 0] 1 1\n",
      "[0 0 0 1 1] 1 1\n",
      "[0 0 1 0 0] 1 1\n",
      "[0 0 1 0 1] 1 1\n",
      "[0 0 1 1 0] 1 1\n",
      "[0 0 1 1 1] 1 1\n",
      "[0 1 0 0 0] 1 1\n",
      "[0 1 0 0 1] 1 1\n",
      "[0 1 0 1 0] 1 1\n",
      "[0 1 0 1 1] 1 1\n",
      "[0 1 1 0 0] 1 1\n",
      "[0 1 1 0 1] 1 1\n",
      "[0 1 1 1 0] 1 1\n",
      "[0 1 1 1 1] 1 1\n",
      "[1 0 0 0 0] 1 1\n",
      "[1 0 0 0 1] 1 1\n",
      "[1 0 0 1 0] 1 1\n",
      "[1 0 0 1 1] 1 1\n",
      "[1 0 1 0 0] 1 1\n",
      "[1 0 1 0 1] 1 1\n",
      "[1 0 1 1 0] 1 1\n",
      "[1 0 1 1 1] 1 1\n",
      "[1 1 0 0 0] 1 1\n",
      "[1 1 0 0 1] 1 1\n",
      "[1 1 0 1 0] 1 1\n",
      "[1 1 0 1 1] 1 1\n",
      "[1 1 1 0 0] 1 1\n",
      "[1 1 1 0 1] 1 1\n",
      "[1 1 1 1 0] 1 1\n",
      "[1 1 1 1 1] 1 1\n"
     ]
    }
   ],
   "source": [
    "for ent, sal in zip(entr,sali):\n",
    "    print(ent,sal,neurona(ent,weight,bias)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 1], [1, 0], [1, 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[x,y] for x in range(0,2) for y in range(0,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
