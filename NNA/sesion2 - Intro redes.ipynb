{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a las redes neuronales\n",
    "1. Neurofisiología elemental\n",
    "1. Evolución histórica, motivación y origen\n",
    "1. Concepto de red neuronal artificial\n",
    "1. Aplicaciones de las redes neuronales\n",
    "\n",
    "## Neurofisiología elemental\n",
    "- El cerebro humano es el sistema mas complejo que conoce el hombre\n",
    "- Realiza diferentes tareas, que permite realizar una comparación computador cerebro\n",
    "\n",
    "El cerebro funciona a a través de las neuronas, células del cerebro. Estas reciben un impulso electrico a través de unas terminales que denominamos _dentritas_. Ese impulso eléctrico se convierten químicamente en el cuerpo de la neurona. Esto se traslada a los _axones_ que tienen la función de enviar impulsos a otras neuronas. Nuestro cerebro tiene unas 86 mil millones de neuronas.\n",
    "\n",
    "**Sinápsis** /\n",
    "La unión entre dos neuronas se conoce como _sinápsis_. Esta se realiza mediante la liberación de sustancias denominadas neurotransmisores. La entrada total de la neurona es la suma de todos los impulsos que recibe.\n",
    "\n",
    "- Cada neurona envía impulsos a otras neuronas y recibe de muchas otras\n",
    "- Pueden existir circuitos de realimentación positiva o negativa\n",
    "- Las conexiones sinápticas pueden ser exitatorias o inhibitorias\n",
    "\n",
    "**Circuitos neuronales y computación**\n",
    "\n",
    "¿Cómo se combinan las capacidades de las neuronas (elementos sencillos) para dar al cerebro enormes capacidades?\n",
    "- La actividad de una neurona es un proceso de todo o nada\n",
    "- Es preciso un número fijo de sinapsis dentro de un periodo de excitación de la neurona\n",
    "- La actividad de cualquier sinapsis inhibitoria impide por completo la excitación\n",
    "- El retorno propio del sistema es el tiempo que requiere la sinapsis\n",
    "- La estructura de la red no cambia en el tiempo. Eso si pasa con nuestro cerebro, en computación no.\n",
    "\n",
    "\n",
    "## Evolución histórica\n",
    "- McCulloch y Pitts en 1943, proponen el modelo de red neuronal con una representación matemática\n",
    "- En 1949, heeb en su libro _The organizacion of behavior_ explica el aprendizaje mediante la modificación de sinapsis. Este libro es la inspiración para los modelos de aprendizaje y sistemas adaptativos.\n",
    "- En 1956 Haibt y Duda, propone la primera simulación bien formulada de red neuronal basada en los postulados de Heeb.\n",
    "- Uttley en 1956 demuestra que una red neuronal con sinapsis cambiante puede aprender a clasificar patrones binarios\n",
    "- En los años 50, se introduce la idea de memoria asociativa una matriz de aprendizaje\n",
    "- Von Newman en su libro _The computer an the Brain_ profundiza las diferencias entre cerebros y computadores\n",
    "- Rossenblat en 1958 introduce el modelo _perceptrón_ para aprendizaje supervisado\n",
    "- En 1960 Widrow y Hoff introduce el modelo Adeline\n",
    "- En 1980 se introducen varios modelos de aprendizaje de neuronas entre ellos el aprendizaje competitivo y las redes Hopfield, que son redes realimentadas\n",
    "- En 1988 Broomhead y Loew describen las redes de funciones de base radial (RBF) que es una alternativa al perceptrón multiplicada\n",
    "- En las dos últimas décadas las redes neuronales han cobrado importancia por su potencia para solucionar problemas de clasificación y potencialidad para aplicaciones de **machine learning**.\n",
    "\n",
    "## Red neuronal artificial\n",
    "\n",
    "- Podemos inspirarnos en el funcionamiento del cerebro para realizar computación\n",
    "- Utilizamos la neurona como elemento de computación\n",
    "- Cada neurona es independiente y funciona como un elemento de procesamiento individual\n",
    "- Cada neurona se conecta con otra (con sus entradas o sus salidas)\n",
    "\n",
    "**Neurona artificial**\n",
    "\n",
    "- Cada neurona tiene unas entradas y unas salidas\n",
    "- Las salidas de unas neuronas pueden ser las entradas de otras neuronas\n",
    "- Utilizamos **conexiones ponderadas**\n",
    "\n",
    "- Si se ajustan adecuadamente los pesos se tendrá un sistema robusto\n",
    "- Puede reconocer ciertos patrones, así estos cambien un poco\n",
    "- Tiene tolerancia al ruido inherente (son modelos robustos)\n",
    "- Las redes neuronales son **soluciones ad-hoc**. Cada problema requiere una red neuronal diferente.\n",
    "\n",
    "**¿Qué vamos a estudiar?**\n",
    "\n",
    "- Representación de la red neuronal (modelo matemático)\n",
    "- Cómo reaccionan las redes a diferentes entradas (funciones de activación)\n",
    "- Estructuración de las redes neuronales (arquitecturas)\n",
    "\n",
    "\n",
    "* **Recomendación:** programa especializado: aprendizaje profundo [Coursera]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propiedades redens neuronales\n",
    "- Aprendizaje adaptativo\n",
    "- Generalización\n",
    "- Naturaleza para propósito no-lineal\n",
    "- Auto-organización\n",
    "- Paralelismo masivo (cada neurona trabaja de forma independiente)\n",
    "- Robustez y tolerancia a ruido\n",
    "\n",
    "## Modelo no lineal\n",
    "\n",
    "- Cada neurona recibe un conjunto de señales discretas o continuas\n",
    "- Estas señales se ponderan o integran\n",
    "- Cada conexión tiene un peso sináptico\n",
    "- Los pesos representan el conocimiento \n",
    "- Estos pesos se ajustan con algoritmos de aprendizaje\n",
    "\n",
    "Una rede neuronal tiene:\n",
    "- Un conjunto _m_ de señales de entrada\n",
    "- un conjunto de sinápsis $w_ij$ donde i indica la i-ésima entrada de la neurona j\n",
    "- Un umbral o sesgo b, puede ser positivo o negativo\n",
    "- Las entradas son sumadas o integradas, tomando en cuenta sus respectivos pesos\n",
    "- Se tienen una función de activación $\\sigma$ que describe el funcionamiento de la neurona\n",
    "\n",
    "El modelo lo podemos escribir así\n",
    "\n",
    "$$z = \\varphi(\\sum_{i=1}^{m} w_{i}x_{i}+b)$$\n",
    "\n",
    "Para i : 'número de neuronas'. En su forma vectorial\n",
    "\n",
    "$$z = \\varphi(WX^{T}+b)$$\n",
    "\n",
    "## Funciones de activación\n",
    "\n",
    "Con una función:\n",
    "1. Función lineal: suele variar entre 0 y 1 o -1 y 1\n",
    "1. Función escalón: salida bivaluada $\\varphi(x) = 0 si x < 0 \\atop 1 si x >= 0 $\n",
    "1. Función sigmoidea: transformación no lineal de la entrada [Es la función mas usada dado que la derivada se escribe en terminos de ella misma]\n",
    "\n",
    "$$\\varphi(x) = \\frac{1}{1+e^{-ax}}$$\n",
    "\n",
    "en esta especificación, suele utilizarse a=1.\n",
    "\n",
    "1. Hay otas que se usan bastante como la tangente hiperbólica. No tan famosa como la sigmoidea.\n",
    "\n",
    "Las salidas están entre 0 (estimulo inhibitorio) y 1 (excitatoria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos una función de activación, en este caso será una función escalon\n",
    "def activacion(x):\n",
    "    return 0 if x < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurona(ent,pesos,bias):\n",
    "    entrada_neta = np.dot(pesos,ent.T)+ bias #Vease la fromulación matemática de una neurona\n",
    "    return activacion(entrada_neta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = np.array([[x,y,z] for x in range(0,2) for y in range(0,2) for z in range(0,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entrada # Combinación de tres para valores binarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32909444  0.64439812 -0.63859375]\n",
      "[-0.28548209]\n"
     ]
    }
   ],
   "source": [
    "# Los pesos deben estar normalizados entre [-1,1], los voy a crear aleatoriamente\n",
    "# Tambien creare el bias\n",
    "\n",
    "pesos = 2*np.random.rand(3)-1\n",
    "print(pesos)\n",
    "\n",
    "bias = 2*np.random.rand(1)-1\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] 0\n",
      "[0 0 1] 0\n",
      "[0 1 0] 1\n",
      "[0 1 1] 0\n",
      "[1 0 0] 1\n",
      "[1 0 1] 0\n",
      "[1 1 0] 1\n",
      "[1 1 1] 1\n"
     ]
    }
   ],
   "source": [
    "# Operamos la neurona: para cada elemento del array, operamos la función neurona e imprimimos el arreglo y la salida\n",
    "for e in entrada:\n",
    "    s = neurona(e,pesos,bias)\n",
    "    print(e,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9734925566817656"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El funcionamiento interno es este\n",
    "1*pesos[0]+1*pesos[1]+0*pesos[2] #es mayor que 0, nos da como salida 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La función de activación puede ser cualquiera, imaginemos la función or\n",
    "\n",
    "salida = list(map(\n",
    "    lambda e: e[0] or e[1]or e[2],\n",
    "    entrada\n",
    "))\n",
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Como mi función es escalonada, debo darle pesos para que reconozca la función or. Esto lo puedo lograr con pesos =1 y un bias =-1\n",
    "#Si al menos un elementos es 1, ya se cumple la función or\n",
    "\n",
    "pesos = np.array([1,1,1]) \n",
    "bias = np.array([-1])\n",
    "\n",
    "for e,t in zip(entrada,salida):\n",
    "    s = neurona(e,pesos,bias)\n",
    "    print(e,t,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] 0 0\n",
      "[0 0 1] 1 0\n",
      "[0 1 0] 1 0\n",
      "[0 1 1] 1 0\n",
      "[1 0 0] 1 0\n",
      "[1 0 1] 1 0\n",
      "[1 1 0] 1 0\n",
      "[1 1 1] 1 1\n"
     ]
    }
   ],
   "source": [
    "# ¿Cómo haríamos ahora para reconocer la función AND?\n",
    "\n",
    "salidaAnd = list(map(\n",
    "    lambda e: e[0] or e[1]or e[2],\n",
    "    entrada\n",
    "))\n",
    "\n",
    "\n",
    "pesosAnd = np.array([1,1,1]) \n",
    "biasAnd = np.array([-3])\n",
    "\n",
    "for e,t in zip(entrada,salidaAnd):\n",
    "    s = neurona(e,pesosAnd,biasAnd) #Acá se ve la importancia del bias\n",
    "    print(e,t,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El aprendizaje\n",
    "\n",
    "El objetivo del entrenamiento de una red es ajustar esos pesos para que se parezca a la salida deseada. Esto se conoce como entrenamiento. Este aprendizaje en las redes neuronales se puede modelar así:\n",
    "\n",
    "$$w(t+1) = w(t) + \\triangle w(t)$$\n",
    "\n",
    "El peso $\\triangle w(t)$ se va ajustando $t$ veces de tal forma que se minimice el error de la salida.\n",
    "\n",
    "Hay diferentes tipos de aprendizajes\n",
    "\n",
    "### Aprendizaje supervisado\n",
    "- basado en la comparación entre la salida actual y la deseada\n",
    "- Los pesos se ajustan de acuerdo a patrón de entrenamiento \n",
    "- Existe un criterio de parada para el procesos de aprendizaje de acuerdo a la medida del error\n",
    "\n",
    "$$E = \\frac{1}{N} \\sum_{p=1}^N (Y_d - Y_c)^2$$\n",
    "\n",
    "### Aprendizaje no supervisado\n",
    "- No hay valores objetivos\n",
    "- Está basado en las correlaciones entre la entrada y patrones significantes en el aprendizaje\n",
    "- Se requiere un método de parada\n",
    "\n",
    "### Aprendizaje por refuerzo\n",
    "- Es un caso especial del aprendizaje supervisado\n",
    "- La salida deseada es desconocida\n",
    "- Se castiga una mala salida y se premia una buena salida (algoritmos genéticos)\n",
    "\n",
    "## Clases de arquitecturas\n",
    "\n",
    "**Redes de una capa sin ciclos**\n",
    "- Es la forma mas simple\n",
    "- Consiste en una capa que recibe la entrada y emite una o más salidas\n",
    "\n",
    "**Multicapa sin ciclos**\n",
    "- Tiene una capa de entrada\n",
    "- Tiene capas ocultas\n",
    "- Tiene capas de salida\n",
    "\n",
    "**Redes recurrentes**\n",
    "- Tiene una esctructura monocapa o multicapa\n",
    "- La salida se conecta a las entradas, pero estas tienen un retardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a programas una neurona con 3 neuronas en la capa de entrada, dos en la primera capa oculta, 3 en la tercera capa oculta y una en la capa de salida. Cada neurona tendrá su propio bias.\n",
    "\n",
    "#Representemos la primera capa oculta (2 neuronas que tiene 3 entradas)\n",
    "\n",
    "activacion_v = np.vectorize(activacion) #Convierte la función de activación en una vactorizada. Así evalúa cada elemento sin el uso de map.\n",
    "\n",
    "def redneuronal(ent,pesosC1,biasC1,\n",
    "                pesosC2,biasC2,\n",
    "                pesosCS,biasCS):\n",
    "    # Capa oculta 1\n",
    "    entC1 = np.dot(pesosC1,ent.T)+ biasC1.T #producto de matrices con np.dot()\n",
    "    salC1 = activacion_v(entC1)\n",
    "    # Capa oculta 2\n",
    "    entC2 = np.dot(pesosC2,salC1)+ biasC2.T\n",
    "    salC2 = activacion_v(entC2)\n",
    "    #Capa de salida\n",
    "    entCS = np.dot(pesosCS,salC2)+biasCS.T\n",
    "    return activacion_v(entCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] [1]\n",
      "[0 0 1] [1]\n",
      "[0 1 0] [1]\n",
      "[0 1 1] [1]\n",
      "[1 0 0] [1]\n",
      "[1 0 1] [1]\n",
      "[1 1 0] [1]\n",
      "[1 1 1] [1]\n"
     ]
    }
   ],
   "source": [
    "# Probemos nuestra red con algunos pesos aleatorios\n",
    "\n",
    "pesosC1 = np.random.rand(2,3)\n",
    "biasC1 = np.random.rand(2)\n",
    "pesosC2 = np.random.rand(3,2)\n",
    "biasC2 = np.random.rand(3)\n",
    "pesosCS = np.random.rand(1,3)\n",
    "biasCS = np.random.rand(1)\n",
    "\n",
    "for i in entrada:\n",
    "    sal = redneuronal(i,pesosC1,biasC1,\n",
    "                pesosC2,biasC2,\n",
    "                pesosCS,biasCS)\n",
    "    print(i,sal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
